{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides Python scripts to import, compile, modify, graph, and export Solinst transducer data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import platform\n",
    "import glob\n",
    "import re\n",
    "import xmltodict\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as dates\n",
    "import matplotlib.ticker as tick\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import statsmodels.tsa.tsatools as tools\n",
    "from pandas.stats.api import ols\n",
    "from datetime import datetime\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Operating System \" + platform.system() + \" \" + platform.release())\n",
    "print(\"Python Version \" + str(sys.version))\n",
    "print(\"Pandas Version \" + str(pd.__version__))\n",
    "print(\"Numpy Version \" + str(np.__version__))\n",
    "print(\"Matplotlib Version \" + str(matplotlib.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rootname = '/media/p/Transcend/PROJECTS/UMAR/Phase_II/Data/RAW/'\n",
    "rootname = 'E:/PROJECTS/UMAR/Data/RAW/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jumpfix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jumpfix(df,meas, threashold=0.005):\n",
    "    '''\n",
    "    removes jumps or jolts in time series data (where offset is lasting)\n",
    "    df = dataframe to manipulate\n",
    "    meas = name of field with jolts\n",
    "    threashold = size of jolt to search for'''\n",
    "    df['delta'+meas] = df.loc[:,meas].diff()\n",
    "    jump = df[abs(df['delta'+meas])>threashold]\n",
    "    jump['cumul'] = jump.loc[:,'delta'+meas].cumsum()\n",
    "    df['newVal'] = df.loc[:,meas]\n",
    "    print jump\n",
    "    for i in range(len(jump)):\n",
    "        jt = jump.index[i]\n",
    "        ja = jump['cumul'][i]\n",
    "        df.loc[jt:,'newVal'] = df[meas].apply(lambda x: x-ja,1)\n",
    "    df[meas]=df['newVal']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getfilename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getfilename(path):\n",
    "    '''\n",
    "    this function extracts the file name without file path or extension\n",
    "    '''\n",
    "    return path.split('\\\\').pop().split('/').pop().rsplit('.', 1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## new_xle_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def new_xle_imp(infile):\n",
    "    '''\n",
    "    This function uses an exact file path to upload a Solinst xle file. \n",
    "    \n",
    "    infile = complete file path to input file\n",
    "    \n",
    "    RETURNS\n",
    "    A pandas dataframe containing the transducer data\n",
    "    '''\n",
    "    # open text file\n",
    "    with open(infile) as fd:\n",
    "        # parse xml\n",
    "        obj = xmltodict.parse(fd.read(),encoding=\"ISO-8859-1\")\n",
    "    # navigate through xml to the data\n",
    "    wellrawdata = obj['Body_xle']['Data']['Log']\n",
    "    # convert xml data to pandas dataframe\n",
    "    f = pd.DataFrame(wellrawdata)\n",
    "    # get header names and apply to the pandas dataframe\n",
    "    f[str(obj['Body_xle']['Ch2_data_header']['Identification']).title()] = f['ch2']\n",
    "    \n",
    "    #CH 3 check\n",
    "    try:\n",
    "        f[str(obj['Body_xle']['Ch3_data_header']['Identification']).title()] = f['ch3']\n",
    "    except(KeyError):\n",
    "        pass\n",
    "    \n",
    "    #CH 2 manipulation\n",
    "    tempunit = (obj['Body_xle']['Ch2_data_header']['Unit'])\n",
    "    if tempunit == 'Deg C' or tempunit == u'\\N{DEGREE SIGN}' + u'C':\n",
    "        f[str(obj['Body_xle']['Ch2_data_header']['Identification']).title()] = pd.to_numeric(f['ch2'])\n",
    "    elif tempunit == 'Deg F' or tempunit == u'\\N{DEGREE SIGN}' + u'F': \n",
    "        f[str(obj['Body_xle']['Ch2_data_header']['Identification']).title()] = pd.to_numeric(f['ch2'])*0.33456\n",
    "\n",
    "    \n",
    "    unit = str(obj['Body_xle']['Ch1_data_header']['Unit']).lower()\n",
    "    if unit == \"feet\" or unit == \"ft\":\n",
    "        f[str(obj['Body_xle']['Ch1_data_header']['Identification']).title()] = pd.to_numeric(f['ch1'])\n",
    "    elif unit == \"kpa\":\n",
    "        f[str(obj['Body_xle']['Ch1_data_header']['Identification']).title()] = pd.to_numeric(f['ch1'])*0.33456\n",
    "    elif unit == \"mbar\":\n",
    "        f[str(obj['Body_xle']['Ch1_data_header']['Identification']).title()] = pd.to_numeric(f['ch1'])*0.0334552565551\n",
    "    elif unit == \"psi\":\n",
    "        f[str(obj['Body_xle']['Ch1_data_header']['Identification']).title()] = pd.to_numeric(f['ch1'])*2.306726\n",
    "    elif unit == \"m\" or unit == \"meters\":\n",
    "        f[str(obj['Body_xle']['Ch1_data_header']['Identification']).title()] = pd.to_numeric(f['ch1'])*3.28084\n",
    "    else:\n",
    "        f[str(obj['Body_xle']['Ch1_data_header']['Identification']).title()] = pd.to_numeric(f['ch1'])\n",
    "        print \"Unknown Units\"\n",
    "        \n",
    "    # add extension-free file name to dataframe\n",
    "    f['name'] = getfilename(infile)\n",
    "    # combine Date and Time fields into one field\n",
    "    f['DateTime'] = pd.to_datetime(f.apply(lambda x: x['Date'] + ' ' + x['Time'], 1))\n",
    "    f[str(obj['Body_xle']['Ch1_data_header']['Identification']).title()] =  pd.to_numeric(f[str(obj['Body_xle']['Ch1_data_header']['Identification']).title()])\n",
    "    f[str(obj['Body_xle']['Ch2_data_header']['Identification']).title()] =  pd.to_numeric(f[str(obj['Body_xle']['Ch2_data_header']['Identification']).title()])\n",
    "    \n",
    "    try:\n",
    "        f[str(obj['Body_xle']['Ch3_data_header']['Identification']).title()] =  pd.to_numeric(f[str(obj['Body_xle']['Ch3_data_header']['Identification']).title()])\n",
    "    except(KeyError):\n",
    "        pass\n",
    "\n",
    "    f = f.reset_index()\n",
    "    f = f.set_index('DateTime')\n",
    "    f = f.drop(['Date','Time','@id','ch1','ch2','index','ms'],axis=1)\n",
    "    \n",
    "    f['MeasuredLevel'] = f['Level'] \n",
    "    return f\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fcl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fcl(df, dtObj):\n",
    "    '''\n",
    "    finds closest date index in a dataframe to a date object\n",
    "    \n",
    "    df = dataframe\n",
    "    dtObj = date object\n",
    "    \n",
    "    taken from: http://stackoverflow.com/questions/15115547/find-closest-row-of-dataframe-to-given-time-in-pandas\n",
    "    '''\n",
    "    return df.iloc[np.argmin(np.abs(df.index.to_pydatetime() - dtObj))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fcl(manualfile, datetime.datetime.strptime('6/3/2015',\"%m/%d/%Y\"))['aw_spc_uS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## manualset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " def manualset(wellbaro,manualfile, manmeas=0, meas=1):\n",
    "    breakpoints = []\n",
    "    bracketedwls = {}\n",
    "\n",
    "    for i in range(len(manualfile)+1):\n",
    "        breakpoints.append(fcl(wellbaro, manualfile.index.to_datetime()[i-1]).name)\n",
    "\n",
    "    last_man_wl,first_man_wl,last_tran_wl,driftlen = [],[],[],[]\n",
    "\n",
    "    for i in range(len(manualfile)-1):\n",
    "        # Break up time series into pieces based on timing of manual measurements\n",
    "        bracketedwls[i+1] = wellbaro.loc[(wellbaro.index.to_datetime() > breakpoints[i+1])&(wellbaro.index.to_datetime() < breakpoints[i+2])]\n",
    "        bracketedwls[i+1].loc[:,'diff_wls'] = bracketedwls[i+1].loc[:,meas].diff() \n",
    "\n",
    "\n",
    "        bracketedwls[i+1].loc[:,'DeltaLevel'] = bracketedwls[i+1].loc[:,meas] - bracketedwls[i+1].ix[0,meas]\n",
    "        bracketedwls[i+1].loc[:,'MeasuredDTW'] = fcl(manualfile,breakpoints[i+1])[manmeas] - bracketedwls[i+1].loc[:,'DeltaLevel']\n",
    "\n",
    "        last_man_wl.append(fcl(manualfile,breakpoints[i+2])[manmeas])\n",
    "        first_man_wl.append(fcl(manualfile,breakpoints[i+1])[manmeas])\n",
    "        last_tran_wl.append(float(bracketedwls[i+1].loc[max(bracketedwls[i+1].index.to_datetime()),'MeasuredDTW']))\n",
    "        driftlen.append(len(bracketedwls[i+1].index))\n",
    "        bracketedwls[i+1].loc[:,'last_diff_int'] = np.round((last_tran_wl[i]-last_man_wl[i]),4)/np.round(driftlen[i]-1.0,4)\n",
    "        bracketedwls[i+1].loc[:,'DriftCorrection'] = np.round(bracketedwls[i+1].loc[:,'last_diff_int'].cumsum()-bracketedwls[i+1].loc[:,'last_diff_int'],4)\n",
    "\n",
    "    wellbarofixed = pd.concat(bracketedwls)\n",
    "    wellbarofixed.reset_index(inplace=True)\n",
    "    wellbarofixed.set_index('DateTime',inplace=True)\n",
    "    # Get Depth to water below casing\n",
    "    wellbarofixed.loc[:,'DTWBelowCasing'] = wellbarofixed['MeasuredDTW'] - wellbarofixed['DriftCorrection']\n",
    "    return wellbarofixed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## smoother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def smoother(df, p, win=30, sd=3):\n",
    "    '''\n",
    "    remove outliers from a pandas dataframe column and fill with interpolated values\n",
    "    warning: this will fill all NaN values in the dataframe with the interpolate function\n",
    "    \n",
    "    INPUT\n",
    "    ------\n",
    "    df= dataframe of interest\n",
    "    p= column in dataframe with outliers\n",
    "    win= size of window\n",
    "    std= number of standard deviations allowed\n",
    "    \n",
    "    RETURNS\n",
    "    ------\n",
    "    Pandas dataframe with outliers removed\n",
    "    '''\n",
    "    df1 = df\n",
    "    df1.loc[:,'dp'+ p] = df1.loc[:,p].diff()\n",
    "    df1.loc[:,'ma'+ p] = pd.rolling_mean(df1.loc[:,'dp'+ p], window=win, center=True)\n",
    "    df1.loc[:,'mst'+p] = pd.rolling_std(df1.loc[:,'dp'+ p], window=win, center=True)\n",
    "    for i in df.index:\n",
    "        try:\n",
    "            if abs(df1.loc[i,'dp'+ p] - df1.loc[i,'ma'+ p]) >= abs(df1.loc[i,'mst'+p]*sd):\n",
    "                df.loc[i,p]=np.nan\n",
    "            else:\n",
    "                df.loc[i,p]=df.loc[i,p]\n",
    "        except (ValueError):\n",
    "            try:\n",
    "                if abs(df1.loc[i,'dp'+ p] - df1.loc[i,'ma'+ p]) >= abs(df1.loc[:,'dp'+p].std()*sd):\n",
    "                    df.loc[i,p]=np.nan\n",
    "                else:\n",
    "                    df.loc[i,p]=df.loc[i,p]\n",
    "            except (ValueError):\n",
    "                df.loc[i,p]=df.loc[i,p]\n",
    "\n",
    "    try:\n",
    "        df1 = df1.drop(['dp'+p,'ma'+p,'mst'+p],axis=1)\n",
    "    except(NameError,ValueError):\n",
    "        pass            \n",
    "    del df1\n",
    "    try:\n",
    "        df = df.drop(['dp'+p,'ma'+p,'mst'+p],axis=1)\n",
    "    except(NameError,ValueError):\n",
    "        pass  \n",
    "    df = df.interpolate(method='time')\n",
    "    df = df[1:-1]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hourly_resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hourly_resample_minutes(df,bse=0,minutes=60):\n",
    "    '''\n",
    "    INPUT\n",
    "    -----\n",
    "    df = pandas dataframe containing time series needing resampling\n",
    "    bse = base time to set; default is zero (on the hour); \n",
    "    minutes = sampling recurrance interval in minutes; default is 60 (hourly samples)\n",
    "    \n",
    "    RETURNS\n",
    "    -----\n",
    "    A pandas dataframe that has been resampled to every hour, at the minute defined by the base (bse)\n",
    "    \n",
    "    DESCRIPTION\n",
    "    -----\n",
    "    see http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.resample.html for more info\n",
    "    \n",
    "    This function uses pandas powerful time-series manipulation to upsample to every minute, then downsample to every hour, \n",
    "    on the hour.\n",
    "    \n",
    "    This function will need adjustment if you do not want it to return hourly samples, or if you are sampling more frequently than\n",
    "    once per minute.\n",
    "    \n",
    "    see http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases\n",
    "    \n",
    "    '''\n",
    "    df = df.resample('1Min') #you can make this smaller to accomodate for a higher sampling frequency\n",
    "    df = df.interpolate(method='time') #http://pandas.pydata.org/pandas-docs/dev/generated/pandas.Series.interpolate.html\n",
    "    df = df.resample(str(minutes)+'Min', how='first',closed='left',label='left', base=bse) #modify '60Min' to change the resulting frequency\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataendclean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dataendclean(df,x):\n",
    "    ## Examine First Value\n",
    "    firstupper = np.mean(df[x].diff()[2:31]) + np.std(df[x].diff()[2:31])*2.2 # 2.2 std dev.\n",
    "    firstlower = np.mean(df[x].diff()[2:31]) - np.std(df[x].diff()[2:31])*2.2 # 2.2 std dev.\n",
    "    firstlev = df[x].diff()[0:2].values[0] # difference of first two values\n",
    "    firstlev1 = df[x].diff(periods=3)[0:4].values[0] # difference of second two values\n",
    "    ## Examine Last Value\n",
    "    lastupper = np.mean(df[x].diff()[-31:-2]) + np.std(df[x].diff()[-31:-2])*2.2 # 2.2 std dev.\n",
    "    lastlower = np.mean(df[x].diff()[-31:-2]) - np.std(df[x].diff()[-31:-2])*2.2 # 2.2 std dev.\n",
    "    lastlev = df[x].diff()[-1:].values[0] # difference of last two values\n",
    "    lastlev1 = df[x].diff(periods=4)[-5:-1].values[0] # difference of last two values\n",
    "\n",
    "    ## drop first value if 2.2 std dev beyond first 30 values\n",
    "    if np.abs(firstlev) > 0.1:\n",
    "        print('jump detected on ' + str(wellname))\n",
    "        if firstlev > firstupper or firstlev < firstlower:\n",
    "            df.drop(df.index[0],inplace=True)\n",
    "            print('drop first on ' + str(wellname))\n",
    "    ## drop last value if 2.2 std dev beyond last 30 values\n",
    "    if np.abs(lastlev) > 0.1:\n",
    "        print('jump detected on ' + str(wellname))\n",
    "        if lastlev > lastupper or lastlev < lastlower:\n",
    "            df.drop(df.index[-1],inplace=True)\n",
    "            print('drop last on ' + str(wellname))\n",
    "    if np.abs(firstlev1) > 0.1:\n",
    "        print('jump detected on ' + str(wellname))\n",
    "        if firstlev1 > firstupper or firstlev1 < firstlower:\n",
    "            df.drop(df.index[0:2],inplace=True)\n",
    "            print('drop first 2 on ' + str(wellname))\n",
    "    ## drop last value if 2.2 std dev beyond last 30 values\n",
    "    if np.abs(lastlev1) > 0.1:\n",
    "        print('jump detected on ' + str(wellname))\n",
    "        if lastlev1 > lastupper or lastlev1 < lastlower:\n",
    "            df.drop(df.index[-10:-1],inplace=True)\n",
    "            print('drop last 2 on ' + str(wellname))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## baro_drift_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def baro_drift_correct(wellfile,barofile,manualfile,sampint=60,wellelev=4800,stickup=0):\n",
    "    '''\n",
    "    INPUT\n",
    "    -----\n",
    "    wellfile = pandas dataframe with water level data labeled 'Level'; index must be datetime\n",
    "    barofile = pandas dataframe with barometric data labeled 'Level'; index must be datetime\n",
    "    manualfile = pandas dataframe with manual level data in the first column after the index; index must be datetime\n",
    "    \n",
    "    sampint = sampling interval in minutes; default 60\n",
    "    wellelev = site ground surface elevation in feet\n",
    "    stickup = offset of measure point from ground in feet\n",
    "    \n",
    "    OUTPUT\n",
    "    -----\n",
    "    wellbarofinal = pandas dataframe with corrected water levels \n",
    "    \n",
    "    This function uses pandas dataframes created using the \n",
    "\n",
    "    '''\n",
    "    #Remove dangling ends\n",
    "    baroclean = dataendclean(barofile, 'Level')\n",
    "    wellclean = dataendclean(wellfile, 'Level')\n",
    "    \n",
    "    # resample data to make sample interval consistent  \n",
    "    baro = hourly_resample(baroclean,0,sampint)\n",
    "    well = hourly_resample(wellclean,0,sampint)\n",
    "    \n",
    "    # reassign `Level` to reduce ambiguity\n",
    "    well['abs_feet_above_levelogger'] = well['Level']\n",
    "    baro['abs_feet_above_barologger'] = baro['Level']\n",
    "    \n",
    "    # combine baro and well data for easy calculations, graphing, and manipulation\n",
    "    wellbaro = pd.merge(well,baro,left_index=True,right_index=True,how='inner')\n",
    "    wellbaro['adjusted_levelogger'] =  wellbaro['abs_feet_above_levelogger'] - wellbaro['abs_feet_above_barologger']\n",
    "    \n",
    "    breakpoints = []\n",
    "    bracketedwls = {}\n",
    "\n",
    "    for i in range(len(manualfile)+1):\n",
    "        breakpoints.append(fcl(wellbaro, manualfile.index.to_datetime()[i-1]).name)\n",
    "\n",
    "    last_man_wl,first_man_wl,last_tran_wl,driftlen = [],[],[],[]\n",
    "\n",
    "    firstupper, firstlower, firstlev, lastupper, lastlower, lastlev = [],[],[],[],[],[]\n",
    "\n",
    "    for i in range(len(manualfile)-1):\n",
    "        # Break up time series into pieces based on timing of manual measurements\n",
    "        bracketedwls[i+1] = wellbaro.loc[(wellbaro.index.to_datetime() > breakpoints[i+1])&(wellbaro.index.to_datetime() < breakpoints[i+2])]\n",
    "        bracketedwls[i+1]['diff_wls'] = bracketedwls[i+1]['abs_feet_above_levelogger'].diff() \n",
    "\n",
    "\n",
    "        bracketedwls[i+1].loc[:,'DeltaLevel'] = bracketedwls[i+1].loc[:,'adjusted_levelogger'] - bracketedwls[i+1].ix[0,'adjusted_levelogger']\n",
    "        bracketedwls[i+1].loc[:,'MeasuredDTW'] = fcl(manualfile,breakpoints[i+1])[0] - bracketedwls[i+1].loc[:,'DeltaLevel']\n",
    "\n",
    "        last_man_wl.append(fcl(manualfile,breakpoints[i+2])[0])\n",
    "        first_man_wl.append(fcl(manualfile,breakpoints[i+1])[0])\n",
    "        last_tran_wl.append(float(bracketedwls[i+1].loc[max(bracketedwls[i+1].index.to_datetime()),'MeasuredDTW']))\n",
    "        driftlen.append(len(bracketedwls[i+1].index))\n",
    "        bracketedwls[i+1].loc[:,'last_diff_int'] = np.round((last_tran_wl[i]-last_man_wl[i]),4)/np.round(driftlen[i]-1.0,4)\n",
    "        bracketedwls[i+1].loc[:,'DriftCorrection'] = np.round(bracketedwls[i+1].loc[:,'last_diff_int'].cumsum()-bracketedwls[i+1].loc[:,'last_diff_int'],4)\n",
    "\n",
    "    wellbarofixed = pd.concat(bracketedwls)\n",
    "    wellbarofixed.reset_index(inplace=True)\n",
    "    wellbarofixed.set_index('DateTime',inplace=True)\n",
    "    # Get Depth to water below casing\n",
    "    wellbarofixed.loc[:,'DTWBelowCasing'] = wellbarofixed['MeasuredDTW'] - wellbarofixed['DriftCorrection']\n",
    "\n",
    "    # subtract casing height from depth to water below casing\n",
    "    wellbarofixed.loc[:,'DTWBelowGroundSurface'] = wellbarofixed.loc[:,'DTWBelowCasing'] - stickup #well riser height\n",
    "\n",
    "    # subtract depth to water below ground surface from well surface elevation\n",
    "    wellbarofixed.loc[:,'WaterElevation'] = wellelev - wellbarofixed.loc[:,'DTWBelowGroundSurface']\n",
    "    \n",
    "    wellbarofinal = smoother(wellbarofixed, 'WaterElevation')\n",
    "    \n",
    "    return wellbarofinal\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Scat(data,bp,wl):\n",
    "    data['dwl'] = data[wl].diff()\n",
    "    data['dbp'] = data[bp].diff()\n",
    "\n",
    "    regression = ols(y=data['dwl'], x=data['dbp'])\n",
    "    m = regression.beta.x\n",
    "    b = regression.beta.intercept\n",
    "    r = regression.r2\n",
    "    #r = (regression.beta.r)**2\n",
    "    plt.scatter(y=data['dwl'], x=data['dbp'])\n",
    "\n",
    "    y_reg = [data['dbp'][i]*m+b for i in range(len(data['dbp']))]\n",
    "\n",
    "    plt.plot(data['dbp'],y_reg, \n",
    "             label='Regression: Y = {m:.4f}X + {b:.5}\\nr^2 = {r:.4f}\\n BE = {be:.2f} '.format(m=m,b=b,r=r,be=m))\n",
    "    plt.legend()\n",
    "    plt.xlabel('Sum of Barometric Pressure Changes (ft)')\n",
    "    plt.ylabel('Sum of Water-Level Changes (ft)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clark's method\n",
    "def clarks(data,bp,wl):\n",
    "    '''\n",
    "    clarks method\n",
    "    Input dataframe (data) with barometric pressure (bp) and water level (wl) data\n",
    "    Returns slope, intercept, and r squared value'''\n",
    "    data['dwl'] = data[wl].diff()\n",
    "    data['dbp'] = data[bp].diff()\n",
    "    \n",
    "    data['beta'] = data['dbp']*data['dwl']\n",
    "    data['Sbp'] = np.abs(data['dbp']).cumsum()\n",
    "    data['Swl'] = data[['dwl','beta']].apply(lambda x: -1*np.abs(x[0]) if x[1]>0 else np.abs(x[0]), axis=1).cumsum()\n",
    "    plt.figure()\n",
    "    plt.plot(data['Sbp'],data['Swl'])\n",
    "    regression = ols(y=data['Swl'], x=data['Sbp'])\n",
    "    \n",
    "    m = regression.beta.x\n",
    "    b = regression.beta.intercept\n",
    "    r = regression.r2\n",
    "    \n",
    "    y_reg = [data.ix[i,'Sbp']*m+b for i in range(len(data['Sbp']))]\n",
    "\n",
    "    plt.plot(data['Sbp'],y_reg,\n",
    "             label='Regression: Y = {m:.4f}X + {b:.5}\\nr^2 = {r:.4f}\\n BE = {be:.2f} '.format(m=m,b=b,r=r,be=m))\n",
    "    plt.legend()\n",
    "    plt.xlabel('Sum of Barometric Pressure Changes (ft)')\n",
    "    plt.ylabel('Sum of Water-Level Changes (ft)')\n",
    "    data.drop(['dwl','dbp','Sbp','Swl'], axis=1, inplace=True)\n",
    "    return m,b,r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## baro_eff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def baro_eff(df,bp,wl,lag=100):\n",
    "    df.dropna(inplace=True)\n",
    "    #dwl = df[wl].diff().values[1:-1]\n",
    "    #dbp = df[bp].diff().values[1:-1]\n",
    "    dwl = np.subtract(df[wl].values[1:-1],np.mean(df[wl].values[1:-1]))\n",
    "    dbp = np.subtract(df[bp].values[1:-1],np.mean(df[bp].values[1:-1]))\n",
    "    df['j_dates'] = df.index.to_julian_date()\n",
    "    lag_time = df['j_dates'].diff().cumsum().values[1:-1]\n",
    "    df.drop('j_dates',axis=1,inplace=True)\n",
    "    # Calculate BP Response Function\n",
    "\n",
    "    ## create lag matrix for regression\n",
    "    bpmat = tools.lagmat(dbp, lag, original='in')\n",
    "    ## transpose matrix to determine required length\n",
    "    ## run least squared regression\n",
    "    sqrd = np.linalg.lstsq(bpmat,dwl)\n",
    "    wlls = sqrd[0]\n",
    "    cumls = np.cumsum(wlls)\n",
    "    negcumls = [-1*cumls[i] for i in range(len(cumls))]\n",
    "    ymod = np.dot(bpmat,wlls)\n",
    "    \n",
    "    ## resid gives the residual of the bp\n",
    "    resid=[(dwl[i] - ymod[i])+np.mean(df[wl].values[1:-1]) for i in range(len(dwl))]\n",
    "    lag_trim = lag_time[0:len(cumls)]\n",
    "    return negcumls, cumls, ymod, resid, lag_time, dwl, dbp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up the Solinst Barologger and Levelogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I always set my transducers to `future start` to make the tranducer start on the hour.  I also allow the Levelogger to take an instantaneous measurement out of water, and zero the transducer out to accomodate for elevation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Relevant Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we must import all of the relevant data. To properly import transducer data, we need:\n",
    "* Transducer (Levelogger) data\n",
    "* Barometric (Barologger) data\n",
    "* Manual Depth to Water Measurements\n",
    "    \n",
    "If we want to calculate water-level elevation, we also need:\n",
    "* Well stickup length (ground to measure point distance)\n",
    "* Ground surface elevation at well\n",
    "<br/>OR<br/>\n",
    "* Elevation of measure point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "barofile = new_xle_imp(rootname + \"baro_2015-07-16.xle\")\n",
    "barofile2 = pd.read_csv(rootname + \"UCC.csv\",parse_dates=True,index_col='Day',skiprows=14, na_values=['M','S'])\n",
    "wellfile = new_xle_imp(rootname +\"arnold_well_2015-07-16.xle\")\n",
    "wellfile2 = new_xle_imp(rootname +\"arnold_well_2015-04-01.xle\")\n",
    "manualfile = pd.read_excel(rootname +\"Manual_Readings.xlsx\",\"Arn_Well\",index_col=\"datetime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "barofile2['ft_water_bp']= barofile2['Sea Level Pressure']*0.0335 - (31.17 - 4806/826 + 7.8) # convert hPa to ft water\n",
    "barofile2 = barofile2.interpolate(method='time') # fill NA spots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Files if Necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatonate the well files so that they are one seamless file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wellfile = pd.concat([wellfile,wellfile2])\n",
    "wellfile.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wellfile.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should always graph raw data to see if there are any tares in the data from users moving the tranducer placement.  Sometimes, the transducer is out of the water when it takes a measurement.  These points should be removed or adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#http://stackoverflow.com/questions/7733693/matplotlib-overlay-plots-with-different-scales\n",
    "x1 = wellfile.index.to_datetime() #converts pandas dataframe index into datetime format for graph\n",
    "x2 = barofile.index.to_datetime()\n",
    "x3 = manualfile.index.to_datetime()\n",
    "\n",
    "y1 = wellfile['Level']\n",
    "y2 = barofile['Level']\n",
    "y3 = manualfile['dtw_ft']\n",
    "\n",
    "data = [(x1,y1),(x2,y2),(x3,y3)]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Twin the x-axis twice to make independent y-axes.\n",
    "axes = [ax, ax.twinx(), ax.twinx()]\n",
    "\n",
    "# Make some space on the right side for the extra y-axis.\n",
    "fig.subplots_adjust(right=0.75)\n",
    "\n",
    "# Move the last y-axis spine over to the right by 20% of the width of the axes\n",
    "axes[-1].spines['right'].set_position(('axes', 1.2))\n",
    "\n",
    "# To make the border of the right-most axis visible, we need to turn the frame\n",
    "# on. This hides the other plots, however, so we need to turn its fill off.\n",
    "axes[-1].set_frame_on(True)\n",
    "axes[-1].patch.set_visible(False)\n",
    "\n",
    "# And finally we get to plot things...\n",
    "colors = ['Green', 'Red', 'Blue']\n",
    "labels = ['Levelogger Pressure (ft)','Barologger Pressure (ft)','Manual Readings (ft to water)' ]\n",
    "marks = ['','','o']\n",
    "linetypes = ['solid','solid','none']\n",
    "\n",
    "for ax, color, datum, label, mark, linety in zip(axes, colors, data, labels, marks, linetypes):\n",
    "    ax.plot(datum[0],datum[1], marker=mark, linestyle=linety, color=color, label=label)\n",
    "    ax.set_ylabel(label, color=color)\n",
    "    ax.tick_params(axis='y', colors=color)\n",
    "    \n",
    "h1, l1 = axes[0].get_legend_handles_labels()\n",
    "h2, l2 = axes[1].get_legend_handles_labels()\n",
    "h3, l3 = axes[2].get_legend_handles_labels()\n",
    "axes[0].legend(h1+h2+h3, l1+l2+l3, loc=4)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print range(-10,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix Jumps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tranducer has a jump in the middle of the data caused by adjustments during manual recordings, as well as a jump at the beginning due to the transducer being out of water at the time of measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wellfile = smoother(wellfile, 'Level', 30, 3)\n",
    "wellfile = smoother(wellfile, 'Conductivity', 30, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wellfile = jumpfix(wellfile,'Level',0.1)\n",
    "wellfile = jumpfix(wellfile,'Conductivity',0.005)\n",
    "wellfile['Level'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Barometric Pressure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solinst transducers are nonvented, meaning that they measure absolute pressure.  When they are submerged in a well, they are measuring the pressure of the water and the atmosphere.  In most cases, we are only interested in the pressure that the water exerts, so we have to subtract the pressure that the atmosphere is exerting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wellbaro = baro_drift_correct(wellfile,barofile,manualfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wellbaro.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wellbaro['WaterElevation'].plot()\n",
    "plt.vlines('11/4/2014 11:16',wellbaro['WaterElevation'].min(),wellbaro['WaterElevation'].max(),color='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Scat(wellbaro,'abs_feet_above_barologger','WaterElevation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s, m, r = clarks(wellbaro,'abs_feet_above_barologger','WaterElevation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "negcumls, cumls, ymod, resid, lag_time, dwl, dbp = baro_eff(wellbaro,'abs_feet_above_barologger','WaterElevation',100)\n",
    "plt.figure()\n",
    "lag_trim = lag_time[0:len(negcumls)]\n",
    "plt.scatter(lag_trim*24,negcumls, label='b.p. alone')\n",
    "plt.xlabel('lag (hours)')\n",
    "plt.ylabel('barometric response')\n",
    "\n",
    "ymin = wellbaro['WaterElevation'].min()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(wellbaro.index[1:-1], resid)\n",
    "plt.text(x='11/3/2014 1:00',y=ymin+2,s='Injection Began',rotation=90,color='green',fontsize=12)\n",
    "y_formatter = tick.ScalarFormatter(useOffset=False)\n",
    "ax.yaxis.set_major_formatter(y_formatter)\n",
    "plt.vlines('11/4/2014 11:16',ymin+3,wellbaro['WaterElevation'].max(),color='green')\n",
    "\n",
    "print len(resid)\n",
    "print len(wellbaro.index[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wellbaro['corrwl'] = wellbaro['WaterElevation'] - wellbaro['abs_feet_above_barologger']*1\n",
    "manualfile['wlelev'] = 4800-manualfile['dtw_ft']\n",
    "\n",
    "x1 = wellbaro.index.to_datetime()[1:-1] #converts pandas dataframe index into datetime format for graph\n",
    "x2 = barofile.index.to_datetime()\n",
    "x3 = manualfile.index.to_datetime()\n",
    "\n",
    "y1 = resid\n",
    "y2 = barofile['Level']\n",
    "y3 = manualfile['wlelev']\n",
    "\n",
    "data = [(x1,y1),(x2,y2),(x3,y3)]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Twin the x-axis twice to make independent y-axes.\n",
    "axes = [ax, ax.twinx(), ax.twinx()]\n",
    "\n",
    "# Make some space on the right side for the extra y-axis.\n",
    "fig.subplots_adjust(right=0.75)\n",
    "\n",
    "# Move the last y-axis spine over to the right by 20% of the width of the axes\n",
    "axes[-1].spines['right'].set_position(('axes', 1.2))\n",
    "\n",
    "# To make the border of the right-most axis visible, we need to turn the frame\n",
    "# on. This hides the other plots, however, so we need to turn its fill off.\n",
    "axes[-1].set_frame_on(True)\n",
    "axes[-1].patch.set_visible(False)\n",
    "\n",
    "# And finally we get to plot things...\n",
    "colors = ['Green', 'Red', 'Blue']\n",
    "labels = ['Levelogger Pressure (ft)','Barologger Pressure (ft)','Manual Readings (ft to water)' ]\n",
    "marks = ['','','o']\n",
    "linetypes = ['solid','solid','none']\n",
    "\n",
    "y_formatter = tick.ScalarFormatter(useOffset=False)\n",
    "\n",
    "for ax, color, datum, label, mark, linety in zip(axes, colors, data, labels, marks, linetypes):\n",
    "    ax.plot(datum[0],datum[1], marker=mark, linestyle=linety, color=color, label=label)\n",
    "    ax.set_ylabel(label, color=color)\n",
    "    ax.tick_params(axis='y', colors=color)\n",
    "    ax.yaxis.set_major_formatter(y_formatter)\n",
    "\n",
    "h1, l1 = axes[0].get_legend_handles_labels()\n",
    "h2, l2 = axes[1].get_legend_handles_labels()\n",
    "h3, l3 = axes[2].get_legend_handles_labels()\n",
    "axes[0].legend(h1+h2+h3, l1+l2+l3, loc=4)\n",
    "axes[2].set_ylim(4485,4493)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match Measurement Interval of Barometer (Barologger) and Transducer  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is best to set Solinst transducers (Leveloggers) to start at the same time and to measure at the same frequency as your Barologger. Sometimes, this does not happen.  To solve mismatches in sampling interval, we can resample the barometer data to same base (start time) and frequency as the transducer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `hourly_resample` function above, we can resample each transducer dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
